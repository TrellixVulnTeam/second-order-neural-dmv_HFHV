from parser.dmvs.dmv import DMV
import torch
from parser.const import *


'''
Latent-variable DMV model. 
Implementation of "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections"
Basically, it treats 'POS tag' as hidden variables, and use additional rules to model the probability of a word generated by a hidden 
POS tag. It marginalizes all latent dependency parse tree and POS tags to get the marginal log-likelihood of word sequences.  
'''
class DMV_lv(DMV):
    def __init__(self, device):
        super(DMV_lv, self).__init__(device)

    @torch.enable_grad()
    def _inside(self, rules, lens=None, viterbi=False, mbr=False, decoding=False):
        attach = rules['attach']
        root_score = rules['root']
        decision = rules['decision']
        unary = rules['unary']
        b, N, states = unary.shape
        attach_left =  attach[:, :, :, LEFT, :] + decision[:, :, None, LEFT, :, GO]
        attach_right =  attach[:, :, :, RIGHT, :] + decision[:, :, None, RIGHT, :, GO]
        if lens is None:
            lens = torch.zeros(b, device=self.device,dtype=torch.long).fill_(N-1)
        else:
            lens = lens - 1
        alpha = [
            [
                [torch.zeros(b, N, N, states, states, VALENCE_NUM, device=self.device).fill_(self.huge) for _ in range(2)
                 ],
                [torch.zeros(b, N, N, states, VALENCE_NUM, device=self.device).fill_(self.huge) for _ in range(2)
                 ]
            ] for _ in range(2)
        ]
        alpha[A][C][L][:, :, 0, :, :] =  unary[:, :, :, None] + decision[:, None, :, LEFT, :, STOP]
        alpha[B][C][L][:, :, -1, :, :] =  unary[:, :, :, None] + decision[:, None, :, LEFT, :, STOP]
        alpha[A][C][R][:, :, 0, :, :] =  decision[:, None, :, RIGHT, :, STOP]
        alpha[B][C][R][:, :, -1, :, :] = decision[:, None, :, RIGHT, :, STOP]
        if decoding:
            arc_indicator = torch.zeros(b, N+1, N+1, device=self.device, requires_grad=True)
        else:
            arc_indicator = torch.zeros(b, N+1, N+1, device=self.device, requires_grad=False)


        semiring_plus = self.get_plus_semiring(viterbi=viterbi)
        for k in range(1, N):
            f = torch.arange(1, N - k + 1), torch.arange(1+ k, N + 1)
            ACL = alpha[A][C][L][:, : N - k, :k]
            ACR = alpha[A][C][R][:,  : N - k, :k]
            BCL = alpha[B][C][L][:, k:, N - k:]
            BCR = alpha[B][C][R][:, k:, N - k :]
            x = ACR[..., None, :,  NOCHILD] + BCL[..., None, HASCHILD]
            arcs_l = semiring_plus(x[..., None] + attach_left[:, None, None, ...]  + arc_indicator[:, f[1], f[0], None, None, None, None], dim=2)
            alpha[A][I][L][:,  : N - k, k] = arcs_l
            alpha[B][I][L][:, k:N, N - k - 1] = arcs_l
            x = ACR[..., None, HASCHILD] + BCL[..., None, :, NOCHILD]
            arcs_r = semiring_plus(x[..., None] + attach_right[:, None, None, ...]  + arc_indicator[:, f[0], f[1], None, None, None, None], dim=2)
            alpha[A][I][R][:, : N - k, k] = arcs_r
            alpha[B][I][R][:, k:N, N - k - 1] = arcs_r
            AIR = alpha[A][I][R][:, : N - k, 1 : k + 1]
            BIL = alpha[B][I][L][:, k:, N - k - 1 : N - 1]
            new_1 = semiring_plus(ACL[..., None, :, NOCHILD, None] + BIL , dim=2)
            new = semiring_plus(new_1, dim=-2)
            alpha[A][C][L][:, : N - k, k] = new
            alpha[B][C][L][:, k:N, N - k - 1] = new
            new_1 = semiring_plus(AIR + BCR[..., None, :, NOCHILD, None], 2)
            new = semiring_plus(new_1, dim=-2)
            alpha[A][C][R][:, : N - k, k] = new
            alpha[B][C][R][:, k:N, N - k - 1] = new
        root_incomplete_span = alpha[A][C][L][:, 0, :, :, NOCHILD] + root_score[:, None, :]
        final = torch.zeros(b, N, device=self.device).fill_(self.huge)
        for k in range(N):
            AIR = root_incomplete_span[:, :k+1]
            BCR = alpha[B][C][R][:, k, N - k -1:]
            new_1 =  semiring_plus(AIR + BCR[..., NOCHILD] + arc_indicator[:, 0, 1:k+2].unsqueeze(-1), dim=1)
            final[:, k] = semiring_plus(new_1, dim=-1)
        logZ = torch.gather(final, 1, lens.unsqueeze(-1))
        if decoding:
            if viterbi:
                arc = torch.autograd.grad(logZ.sum(), arc_indicator)[0].nonzero()
                predicted = torch.zeros(b, N, device=self.device, dtype=torch.long).fill_(-1)
                predicted[arc[:, 0], arc[:, 2]-1] = arc[:, 1]
                return {'predicted_arc': predicted}
            elif mbr:
                arc_marginal = torch.autograd.grad(logZ.sum(), arc_indicator)[0]
                predicted = self._eisner(arc_marginal, lens)
                return {'predicted_arc': predicted}
            else:
                raise NotImplementedError
        return {'partition': logZ}
